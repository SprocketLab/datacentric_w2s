{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "sys.path.append(\"/u/c/h/chshin/changho/datacentric_w2s\")\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ruptures import Binseg\n",
    "from datasets import DatasetDict, load_from_disk\n",
    "from simple_parsing import parse\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from w2s.ds_registry import load_and_process_dataset\n",
    "from w2s.model import ModelConfig\n",
    "from w2s.sft import train, linear_probe_train, load_model_and_predict, load_model_and_save_activations\n",
    "from w2s.sft_config import SFTConfig\n",
    "from w2s.probe import ProbeConfig, LogisticProbeConfig\n",
    "from w2s.utils import get_config_foldername\n",
    "from simple_parsing import Serializable, field, subgroups\n",
    "from w2s.ds_registry import VALID_DATASETS\n",
    "from w2s.probe import PROBES\n",
    "from w2s.data_source_selection_linear_probing import (\n",
    "    detect_and_partition,\n",
    "    detect_hard_nonhard,\n",
    "    detect_overlap_easy,\n",
    "    partition_indices_with_ratio,\n",
    "    setup_arms,\n",
    "    StrategyTracker\n",
    ")\n",
    "import argparse\n",
    "\n",
    "seed = 0\n",
    "dataset_name = \"sciq\"\n",
    "\n",
    "if os.path.exists(f'../../results/data_selection_linear_probing_overlap_only_eval/data_selection_with_linear_probing_overlap_only_results_{dataset_name}_{seed}.json'):\n",
    "    print(\"The result exists already!\")\n",
    "    print(f\"Loading results from ../../results/data_selection_linear_probing_overlap_only_eval/data_selection_with_linear_probing_overlap_only_results_{dataset_name}_{seed}.json\")\n",
    "    with open(f'../../results/data_selection_linear_probing_overlap_only_eval/data_selection_with_linear_probing_overlap_only_results_{dataset_name}_{seed}.json', 'r') as f:\n",
    "        result = json.load(f)\n",
    "    print(result)\n",
    "    exit()\n",
    "else:\n",
    "        \n",
    "\n",
    "    probe_name = \"logreg\"\n",
    "    probe_cfg = LogisticProbeConfig()\n",
    "    # dataset_name = \"sciq\"\n",
    "    df_result = []\n",
    "    data_stats = []\n",
    "\n",
    "    overlap_sampling_ratio_list = [0.1, 0.9]\n",
    "    K = len(overlap_sampling_ratio_list)\n",
    "    hard_sampling_ratio_list = [1/K for _ in range(K)]\n",
    "    easy_sampling_ratio_list = [1/K for _ in range(K)]\n",
    "    T = 50\n",
    "\n",
    "    cfg = SFTConfig(\n",
    "        dataset=dataset_name,\n",
    "        # n_train=500,\n",
    "        # n_val=100,\n",
    "        # n_test=300,\n",
    "        n_train=10_000,\n",
    "        n_val=1_000,\n",
    "        n_test=5_000,\n",
    "        n_predict=0,\n",
    "        minibatch_size=1,\n",
    "        batch_size=32,\n",
    "        results_folder=\"../../results\",\n",
    "        seed=seed,\n",
    "        disable_lora=True,\n",
    "        strong_only=True,\n",
    "        probe=LogisticProbeConfig(),\n",
    "            run_name=f\"{dataset_name}_{seed}\",\n",
    "        )\n",
    "\n",
    "    root = Path(cfg.results_folder) / cfg.run_name\n",
    "    shared_root = Path(cfg.results_folder) / cfg.shared_folder\n",
    "    cfg_name = f\"{cfg.run_name}_{cfg.weak_model_name.split('/')[-1]}_{cfg.strong_model_name.split('/')[-1]}\"\n",
    "\n",
    "    # Save splits first\n",
    "    save_path = shared_root / cfg_name / \"splits\"\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading splits from {save_path}\")\n",
    "        splits = load_from_disk(str(save_path)) \n",
    "    else:\n",
    "        print(f\"Loading and processing dataset {cfg.dataset}\")\n",
    "        splits = load_and_process_dataset(\n",
    "            cfg.dataset, cfg.n_train, cfg.n_val, cfg.n_test, cfg.n_predict\n",
    "        )\n",
    "\n",
    "        train_halves = splits[\"train\"].train_test_split(test_size=0.5, seed=cfg.seed)\n",
    "        splits[\"weak_train\"] = train_halves[\"train\"]\n",
    "        splits[\"strong_train\"] = train_halves[\"test\"]\n",
    "\n",
    "        cols = [\"hard_label\", \"txt\"]\n",
    "        splits = splits.select_columns(cols).rename_column(\"hard_label\", \"labels\")\n",
    "        for split in splits:\n",
    "            splits[split] = splits[split].add_column(\"gt_labels\", splits[split][\"labels\"])\n",
    "\n",
    "        print(\n",
    "            f\"Example:\\n\\n{splits['strong_train'][0]['txt']}\\n\\nLabel: {splits['strong_train'][0]['labels']}\"\n",
    "        )\n",
    "\n",
    "\n",
    "        print(f\"Saving splits to {save_path}\")\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        splits.save_to_disk(str(save_path))\n",
    "\n",
    "\n",
    "\n",
    "    weak_train_args: dict = dict(\n",
    "        num_train_epochs=cfg.n_epochs,\n",
    "        adam_beta2=0.95,\n",
    "        gradient_accumulation_steps=cfg.batch_size // cfg.minibatch_size,\n",
    "        eval_strategy=\"steps\",\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=cfg.load_best_model_at_end,\n",
    "        logging_steps=25,\n",
    "        metric_for_best_model=cfg.metric_for_best_model,\n",
    "        greater_is_better=cfg.greater_is_better,\n",
    "        per_device_train_batch_size=cfg.minibatch_size,\n",
    "        per_device_eval_batch_size=cfg.minibatch_size,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=cfg.save_total_limit,\n",
    "        tf32=True,  # Use Tensor Cores even for fp32 matmuls\n",
    "        warmup_steps=cfg.n_warmup_steps,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        lr_scheduler_type=cfg.lr_schedule,\n",
    "        eval_steps=cfg.eval_every,\n",
    "        save_steps=cfg.save_every,\n",
    "    )\n",
    "\n",
    "    strong_train_args = dict(\n",
    "        num_train_epochs=cfg.n_epochs,\n",
    "        adam_beta2=0.95,\n",
    "        gradient_accumulation_steps=cfg.batch_size // cfg.minibatch_size,\n",
    "        eval_strategy=\"steps\",\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=cfg.load_best_model_at_end,\n",
    "        logging_steps=25,\n",
    "        metric_for_best_model=cfg.metric_for_best_model,\n",
    "        greater_is_better=cfg.greater_is_better,\n",
    "        per_device_train_batch_size=cfg.minibatch_size,\n",
    "        per_device_eval_batch_size=cfg.minibatch_size,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=cfg.save_total_limit,\n",
    "        tf32=True,  # Use Tensor Cores even for fp32 matmuls\n",
    "        warmup_steps=cfg.n_warmup_steps,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        lr_scheduler_type=cfg.lr_schedule,\n",
    "        eval_steps=cfg.eval_every,\n",
    "        save_steps=cfg.save_every,\n",
    "    )\n",
    "\n",
    "    w2s_train_args = dict(\n",
    "        num_train_epochs=cfg.n_epochs,\n",
    "        adam_beta2=0.95,\n",
    "        gradient_accumulation_steps=cfg.batch_size // cfg.minibatch_size,\n",
    "        eval_strategy=\"steps\",\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=cfg.load_best_model_at_end,\n",
    "        logging_steps=25,\n",
    "        metric_for_best_model=cfg.metric_for_best_model,\n",
    "        greater_is_better=cfg.greater_is_better,\n",
    "        per_device_train_batch_size=cfg.minibatch_size,\n",
    "        per_device_eval_batch_size=cfg.minibatch_size,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=cfg.save_total_limit,\n",
    "        tf32=True,  # Use Tensor Cores even for fp32 matmuls\n",
    "        warmup_steps=cfg.n_warmup_steps,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        lr_scheduler_type=cfg.lr_schedule,\n",
    "        eval_steps=cfg.eval_every,\n",
    "        save_steps=cfg.save_every,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    def get_model_and_run_name(model_name, current_name):\n",
    "        model_last = model_name.split(\"/\")[-1]\n",
    "        model_cfg = ModelConfig(name=model_name, enable_lora=not cfg.disable_lora)\n",
    "        run_name = f\"{current_name}-{cfg.run_name}-{cfg.dataset}-{model_last}\"\n",
    "        return model_cfg, run_name\n",
    "\n",
    "    # train weak floor, get predictions\n",
    "    print(\"\\n\\033[32m===== Linear probing experiments =====\\033[0m\")\n",
    "    weak_model_cfg, weak_run_name = get_model_and_run_name(cfg.weak_model_name, \"weak\")\n",
    "    weak_train_args[\"run_name\"] = weak_run_name\n",
    "    weak_train_args[\"output_dir\"] = str(shared_root / cfg_name / \"weak\")\n",
    "    weak_train_args[\"learning_rate\"] = cfg.weak_lr\n",
    "\n",
    "    w2s_model_cfg, w2s_run_name = get_model_and_run_name(cfg.strong_model_name, \"strong\")\n",
    "    w2s_train_args[\"run_name\"] = w2s_run_name\n",
    "    w2s_train_args[\"output_dir\"] = str(shared_root / cfg_name / \"w2s\")\n",
    "    w2s_train_args[\"learning_rate\"] = cfg.strong_lr\n",
    "\n",
    "    # Do sampling for faster check\n",
    "    w2s_train = splits[\"strong_train\"]\n",
    "    w2s_val = splits[\"val\"]\n",
    "\n",
    "    weak_ds_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": splits[\"weak_train\"],\n",
    "            \"val\": splits[\"val\"],\n",
    "            \"test\": splits[\"test\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    strong_ds_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": w2s_train,\n",
    "            \"val\": w2s_val,\n",
    "            \"test\": splits[\"test\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    weak_acts_dir = shared_root / cfg_name / \"weak_activations\"\n",
    "    strong_acts_dir = shared_root / cfg_name / \"strong_activations\"\n",
    "\n",
    "    x_weak_train = torch.load(weak_acts_dir / f\"weak_train.pt\", map_location=\"cuda\")\n",
    "    x_strong_train = torch.load(strong_acts_dir / f\"strong_train.pt\", map_location=\"cuda\")\n",
    "    x_w2s_train_for_pseudolabeling = torch.load(weak_acts_dir / f\"strong_train.pt\", map_location=\"cuda\")\n",
    "    x_weak_test = torch.load(weak_acts_dir / f\"test.pt\", map_location=\"cuda\")\n",
    "    x_strong_test = torch.load(strong_acts_dir / f\"test.pt\", map_location=\"cuda\")\n",
    "    y_weak_train = torch.tensor(splits[\"weak_train\"][\"labels\"], device=\"cuda\")\n",
    "    y_strong_train = torch.tensor(splits[\"strong_train\"][\"labels\"], device=\"cuda\")\n",
    "    y_test = torch.tensor(splits[\"test\"][\"labels\"], device=\"cuda\")\n",
    "\n",
    "\n",
    "    print(f\"Weak acts shape: {x_weak_train.shape}\")\n",
    "    print(f\"Strong acts shape: {x_strong_train.shape}\")\n",
    "\n",
    "    weak_model = PROBES[probe_name](probe_cfg)\n",
    "    weak_model.fit(x_weak_train, y_weak_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_w2s_train_for_pseudolabeling = weak_model.predict(x_w2s_train_for_pseudolabeling)\n",
    "\n",
    "    # Compute accuracy for weak probe on test set\n",
    "    with torch.no_grad():\n",
    "        weak_preds = weak_model.predict(x_weak_test)\n",
    "        weak_test_labels = torch.tensor(splits[\"test\"][\"labels\"], device=\"cuda\")\n",
    "    weak_test_accuracy = (weak_preds.round() == weak_test_labels).float().mean().item()\n",
    "\n",
    "    y_w2s_train_for_pseudolabeling = y_w2s_train_for_pseudolabeling.cpu().detach().numpy()\n",
    "    x_strong_train = x_strong_train.cpu().detach().numpy()\n",
    "    y_strong_train = y_strong_train.cpu().detach().numpy()\n",
    "\n",
    "    detected_hard_indices, detected_nonhard_indices = detect_hard_nonhard(x_w2s_train_for_pseudolabeling, y_w2s_train_for_pseudolabeling)\n",
    "\n",
    "\n",
    "    x_w2s_train_hard = x_strong_train[detected_hard_indices]\n",
    "    y_w2s_train_hard = y_strong_train[detected_hard_indices]\n",
    "    x_w2s_train_nonhard = x_strong_train[detected_nonhard_indices]\n",
    "    y_w2s_train_nonhard = y_strong_train[detected_nonhard_indices]\n",
    "    detected_easy_indices, detected_overlap_indices = detect_overlap_easy(x_w2s_train_nonhard, x_w2s_train_hard, detected_nonhard_indices)\n",
    "    x_w2s_train_overlap = x_strong_train[detected_overlap_indices]\n",
    "    y_w2s_train_overlap = y_strong_train[detected_overlap_indices]\n",
    "    x_w2s_train_nonoverlap = np.concatenate([x_strong_train[detected_easy_indices], x_strong_train[detected_hard_indices]])\n",
    "    y_w2s_train_nonoverlap = np.concatenate([y_strong_train[detected_easy_indices], y_strong_train[detected_hard_indices]])\n",
    "\n",
    "    # Random partition x_strong based on overlap sampling ratio\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(detected_easy_indices)\n",
    "    np.random.shuffle(detected_hard_indices)\n",
    "    np.random.shuffle(detected_overlap_indices)\n",
    "\n",
    "    detected_overlap_partition = partition_indices_with_ratio(detected_overlap_indices, overlap_sampling_ratio_list, seed)\n",
    "    detected_easy_partition = partition_indices_with_ratio(detected_easy_indices, easy_sampling_ratio_list, seed)\n",
    "    detected_hard_partition = partition_indices_with_ratio(detected_hard_indices, hard_sampling_ratio_list, seed)\n",
    "\n",
    "    # Loop to decie number_of_samples_per_round\n",
    "    sample_size_list = []\n",
    "    overlap_density_list = []\n",
    "    for easy_indices, hard_indices, overlap_indices in zip(detected_easy_partition, detected_hard_partition, detected_overlap_partition):\n",
    "        sample_indices = np.concatenate([easy_indices, hard_indices, overlap_indices])\n",
    "        sample_size_list.append(sample_indices.shape[0])\n",
    "        overlap_density_list.append(overlap_indices.shape[0] / sample_indices.shape[0])\n",
    "    min_sample_size = min(sample_size_list)\n",
    "    number_of_samples_per_round = int(min_sample_size / T)\n",
    "\n",
    "\n",
    "    ############## Bandit #############\n",
    "    # Setup sources\n",
    "    print(\"Setting up sources...\")\n",
    "    arms = setup_arms(K, detected_easy_partition, detected_hard_partition, detected_overlap_partition, x_strong_train, x_w2s_train_for_pseudolabeling, y_strong_train, overlap_density_list, number_of_samples_per_round, T, seed)\n",
    "    opt_o_log = []\n",
    "    opt_acc_overlap_only_log = []\n",
    "    ucb_o_log = []\n",
    "    ucb_acc_overlap_only_log = []\n",
    "    ucb_ucb_log =[]\n",
    "    random_o_log = []\n",
    "    random_acc_overlap_only_log = []\n",
    "\n",
    "    # Run Bandit: Optimal\n",
    "    print(\"Running Optimal...\")\n",
    "\n",
    "\n",
    "\n",
    "    opt_source = np.argmax(overlap_sampling_ratio_list)\n",
    "    opt_tracker = StrategyTracker(x_strong_test, y_test)\n",
    "\n",
    "    for i in tqdm(range(T)):\n",
    "        source = opt_source\n",
    "        arm = arms[source]\n",
    "        X_train_sample, X_weak_train_sample, y_train_sample = arm.get_samples()\n",
    "        with torch.no_grad():\n",
    "            y_train_sample_pseudo = weak_model.predict(X_weak_train_sample)\n",
    "        \n",
    "        partition_dict = detect_and_partition(X_train_sample, y_train_sample, y_train_sample_pseudo, opt_tracker)\n",
    "        arm.update(partition_dict)\n",
    "        \n",
    "        opt_tracker.update_samples(partition_dict)\n",
    "        opt_o_log.append(opt_tracker.get_est_overlap_ratio())\n",
    "        opt_acc_overlap_only_log.append(opt_tracker.train_and_eval_overlap_only())\n",
    "\n",
    "    ### UCB\n",
    "    arms = setup_arms(K, detected_easy_partition, detected_hard_partition, detected_overlap_partition, x_strong_train, x_w2s_train_for_pseudolabeling, y_strong_train, overlap_density_list, number_of_samples_per_round, T, seed)\n",
    "    ucb_tracker = StrategyTracker(x_strong_test, y_test)\n",
    "\n",
    "    for i in tqdm(range(K)):\n",
    "        source = i\n",
    "        arm = arms[source]\n",
    "        X_train_sample, X_weak_train_sample, y_train_sample = arm.get_samples()\n",
    "        with torch.no_grad():\n",
    "            y_train_sample_pseudo = weak_model.predict(X_weak_train_sample)\n",
    "        \n",
    "        partition_dict = detect_and_partition(X_train_sample, y_train_sample, y_train_sample_pseudo, ucb_tracker)\n",
    "        arm.update(partition_dict)\n",
    "        ucb_tracker.update_samples(partition_dict)\n",
    "        ucb_o_log.append(ucb_tracker.get_est_overlap_ratio())\n",
    "        ucb_acc_overlap_only_log.append(ucb_tracker.train_and_eval_overlap_only())\n",
    "    for i in tqdm(range(T-K)):\n",
    "        ucbs = []\n",
    "        for k in range(K):\n",
    "            ucbs.append(arms[k].compute_ucb())\n",
    "        ucb_ucb_log.append(ucbs)\n",
    "        source = np.argmax(ucbs)\n",
    "        arm = arms[source]\n",
    "        X_train_sample, X_weak_train_sample, y_train_sample = arm.get_samples()\n",
    "        with torch.no_grad():\n",
    "            y_train_sample_pseudo = weak_model.predict(X_weak_train_sample)\n",
    "        \n",
    "        partition_dict = detect_and_partition(X_train_sample, y_train_sample, y_train_sample_pseudo, ucb_tracker)\n",
    "        \n",
    "        arm.update(partition_dict)\n",
    "        ucb_tracker.update_samples(partition_dict)\n",
    "        ucb_o_log.append(ucb_tracker.get_est_overlap_ratio())\n",
    "        ucb_acc_overlap_only_log.append(ucb_tracker.train_and_eval_overlap_only())\n",
    "\n",
    "    ### Random\n",
    "    print(\"Running Random...\")\n",
    "    arms = setup_arms(K, detected_easy_partition, detected_hard_partition, detected_overlap_partition, x_strong_train, x_w2s_train_for_pseudolabeling, y_strong_train, overlap_density_list, number_of_samples_per_round, T, seed)\n",
    "    random_tracker = StrategyTracker(x_strong_test, y_test)\n",
    "\n",
    "    for i in tqdm(range(T)):\n",
    "        source = np.random.randint(K)\n",
    "        arm = arms[source]\n",
    "        X_train_sample, X_weak_train_sample, y_train_sample = arm.get_samples()\n",
    "        with torch.no_grad():\n",
    "            y_train_sample_pseudo = weak_model.predict(X_weak_train_sample)\n",
    "        partition_dict = detect_and_partition(X_train_sample, y_train_sample, y_train_sample_pseudo, random_tracker)\n",
    "        arm.update(partition_dict)\n",
    "        random_tracker.update_samples(partition_dict)\n",
    "        random_o_log.append(random_tracker.get_est_overlap_ratio())\n",
    "        random_acc_overlap_only_log.append(random_tracker.train_and_eval_overlap_only())\n",
    "\n",
    "    result = {\n",
    "        'seed': seed,\n",
    "        'dataset_name': dataset_name,\n",
    "        'opt_o_log': opt_o_log,\n",
    "        'ucb_o_log': ucb_o_log,\n",
    "        'random_o_log': random_o_log,\n",
    "        'opt_acc_overlap_only_log': opt_acc_overlap_only_log,\n",
    "        'ucb_acc_overlap_only_log': ucb_acc_overlap_only_log,\n",
    "        'random_acc_overlap_only_log': random_acc_overlap_only_log,\n",
    "        'ucb_ucb_log': ucb_ucb_log,\n",
    "        'overlap_density_list': overlap_density_list,\n",
    "        'min_sample_size': min_sample_size,\n",
    "        'number_of_samples_per_round': number_of_samples_per_round,\n",
    "    }\n",
    "    df_result.append(result)\n",
    "    print(result)\n",
    "\n",
    "    if not os.path.exists(f'../../results/data_selection_linear_probing_overlap_only_eval/'):\n",
    "        os.makedirs(f'../../results/data_selection_linear_probing_overlap_only_eval/')\n",
    "\n",
    "    with open(f'../../results/data_selection_linear_probing_overlap_only_eval/data_selection_with_linear_probing_overlap_only_results_{dataset_name}_{seed}.json', 'w') as f:\n",
    "        json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacentric_w2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
