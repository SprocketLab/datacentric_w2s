{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from ruptures import Binseg\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "from src.data_util import load_wrench_dataset, load_LF\n",
    "from src.metrics import exp_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_baseline_pseudolabel(L, seed=123, y_train=None):\n",
    "    \n",
    "    label_model = LabelModel(cardinality=2, verbose=False)\n",
    "    label_model.fit(L_train=L,\n",
    "                    n_epochs=1000, log_freq=100, seed=seed)\n",
    "    y_train_pseudo = label_model.predict(L, tie_break_policy=\"random\")  \n",
    "    \n",
    "    return y_train_pseudo\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32, output_dim=2):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "    def get_last_layer_activation(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def train_dnn(X, y, X_test, y_test, epochs=10, batch_size=32, verbose=False):\n",
    "    # Remove scaling\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = DNN(input_dim=X.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss_sum/len(dataloader)}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor).argmax(dim=1).numpy()\n",
    "    if verbose:\n",
    "        print('y_pred, y_test', y_pred, y_test, np.sum(y_pred), np.sum(y_test), np.sum(y_pred == y_test))\n",
    "    return model, accuracy_score(y_test, y_pred)\n",
    "\n",
    "def get_dnn_last_layer_activation(model, X):\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        activations = model.get_last_layer_activation(X_tensor).numpy()\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "verbose = False\n",
    "dataset_name_list =  ['basketball', 'cdr', 'census', 'commercial', 'imdb', 'mushroom', 'sms', 'spambase', 'tennis', 'yelp', 'youtube']\n",
    "data_base_path = '/home/changho/datasets'\n",
    "df_result = []\n",
    "for seed in range(25):\n",
    "        \n",
    "    for dataset_name in dataset_name_list:\n",
    "        try:\n",
    "                \n",
    "            x_train, y_train, x_test, y_test = load_wrench_dataset(dataset_name=dataset_name,\n",
    "                                                                data_base_path=data_base_path)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "            indices_train = np.arange(len(x_train))\n",
    "            weak_train_indices, w2s_train_indices = train_test_split(indices_train, test_size=0.5,\n",
    "                                                                    random_state=seed)\n",
    "            \n",
    "            x_weak_train, y_weak_train = x_train[weak_train_indices], y_train[weak_train_indices]\n",
    "            x_w2s_train, y_w2s_train = x_train[w2s_train_indices], y_train[w2s_train_indices]\n",
    "\n",
    "            L = load_LF(dataset_name, data_base_path=data_base_path)\n",
    "            L_weak_train = L[weak_train_indices]\n",
    "            L_w2s_train = L[w2s_train_indices]\n",
    "\n",
    "            # Stage 1. Train Weak Label Model\n",
    "            label_model = LabelModel(cardinality=2, verbose=False)\n",
    "            label_model.fit(L_train=L_weak_train, n_epochs=1000, log_freq=100, seed=seed)\n",
    "\n",
    "            # Stage 2. Separate hard points from easy or overlap points\n",
    "            proba_w2s_train = label_model.predict_proba(L_w2s_train)[:, 1]\n",
    "            confidence_w2s_train = 2*np.abs(proba_w2s_train - 0.5)\n",
    "\n",
    "\n",
    "            # Get indices of points in the cluster with minimum center\n",
    "            # Apply change point detection to decide threshold\n",
    "            \n",
    "\n",
    "            # Sort confidence scores\n",
    "            sorted_confidence = np.sort(confidence_w2s_train)\n",
    "            \n",
    "            # Perform change point detection\n",
    "            model = Binseg(model=\"l2\").fit(sorted_confidence.reshape(-1, 1))\n",
    "            change_points = model.predict(n_bkps=1)[0]\n",
    "            \n",
    "            # Use the detected change point as the threshold\n",
    "            confidence_threshold = sorted_confidence[change_points]\n",
    "            \n",
    "            # Get indices of points below the threshold\n",
    "            lower_confidence_indices = np.where(confidence_w2s_train <= confidence_threshold)[0]\n",
    "            high_confidence_indices = np.where(confidence_w2s_train > confidence_threshold)[0]\n",
    "            \n",
    "            x_hard_points = x_w2s_train[lower_confidence_indices]\n",
    "            L_hard_points = L_w2s_train[lower_confidence_indices]\n",
    "            y_hard_points = y_w2s_train[lower_confidence_indices]\n",
    "\n",
    "            x_easy_or_overlap = x_w2s_train[high_confidence_indices]\n",
    "            L_easy_or_overlap = L_w2s_train[high_confidence_indices]\n",
    "            y_easy_or_overlap = y_w2s_train[high_confidence_indices]\n",
    "\n",
    "            # Train DNN on pseudolabeled dataset\n",
    "            X_pseudolabeled = x_w2s_train\n",
    "            y_pseudolabeled = label_model.predict(L_w2s_train, tie_break_policy=\"random\")\n",
    "            \n",
    "            model_pseudo, _ = train_dnn(X_pseudolabeled, y_pseudolabeled, x_test, y_test, verbose=verbose)\n",
    "            \n",
    "            # Get activations for X_combined (which is x_easy_or_overlap)\n",
    "            activations_easy_or_overlap = get_dnn_last_layer_activation(model_pseudo, x_easy_or_overlap)\n",
    "            \n",
    "            # Get activations for x_hard_points\n",
    "            activations_hard_points = get_dnn_last_layer_activation(model_pseudo, x_hard_points)\n",
    "            \n",
    "            # Compute align scores using inner products of activations\n",
    "            activations_easy_or_overlap_normalized = activations_easy_or_overlap / np.linalg.norm(activations_easy_or_overlap, axis=1, keepdims=True)\n",
    "            activations_hard_points_normalized = activations_hard_points / np.linalg.norm(activations_hard_points, axis=1, keepdims=True)\n",
    "            align_scores = np.abs(activations_easy_or_overlap_normalized @ activations_hard_points_normalized.T).mean(axis=1)\n",
    "            \n",
    "            \n",
    "            # Apply change point detection to decide threshold for align scores\n",
    "            sorted_align_scores = np.sort(align_scores)\n",
    "            \n",
    "            # Perform change point detection\n",
    "            model = Binseg(model=\"l2\").fit(sorted_align_scores.reshape(-1, 1))\n",
    "            change_points = model.predict(n_bkps=1)[0]\n",
    "            \n",
    "            # Use the detected change point as the threshold\n",
    "            align_score_threshold = sorted_align_scores[change_points]\n",
    "            \n",
    "            # Get indices of points above the threshold\n",
    "            overlap_indices = np.where(align_scores >= align_score_threshold)[0]\n",
    "            nonoverlap_indices = np.where(align_scores < align_score_threshold)[0]\n",
    "            \n",
    "            # Select the corresponding data points\n",
    "            x_overlap = x_easy_or_overlap[overlap_indices]\n",
    "            L_overlap = L_easy_or_overlap[overlap_indices]\n",
    "            y_overlap_pseudo = label_model.predict(L_overlap, tie_break_policy=\"random\")\n",
    "            y_overlap = y_easy_or_overlap[overlap_indices]\n",
    "            overlap_avg_acc = np.mean(y_overlap_pseudo == y_overlap)\n",
    "            overlap_avg_confidence = np.mean(confidence_w2s_train[overlap_indices])\n",
    "\n",
    "            x_nonoverlap = x_easy_or_overlap[nonoverlap_indices]\n",
    "            L_nonoverlap = L_easy_or_overlap[nonoverlap_indices]\n",
    "            y_nonoverlap_pseudo = label_model.predict(L_nonoverlap, tie_break_policy=\"random\")\n",
    "            y_nonoverlap = y_easy_or_overlap[nonoverlap_indices]\n",
    "            nonoverlap_avg_acc = np.mean(y_nonoverlap_pseudo == y_nonoverlap)\n",
    "            nonoverlap_avg_confidence = np.mean(confidence_w2s_train[nonoverlap_indices])\n",
    "\n",
    "            # weak model evaluation\n",
    "            y_pred = label_model.predict(L_w2s_train, tie_break_policy=\"random\")\n",
    "            lm_acc = np.mean(y_pred == y_w2s_train)\n",
    "\n",
    "\n",
    "            # Run mixing experiments\n",
    "            acc_list = []\n",
    "            wl_dataset_size = min(len(x_overlap), len(x_nonoverlap))\n",
    "            w2s_train_indices = np.arange(len(x_w2s_train))\n",
    "            w2s_train_sampled_indices = np.random.choice(w2s_train_indices, size=wl_dataset_size, replace=False)\n",
    "            x_w2s_train_sampled = x_w2s_train[w2s_train_sampled_indices]\n",
    "            y_w2s_train_sampled = y_w2s_train[w2s_train_sampled_indices]\n",
    "            model, gt_acc = train_dnn(x_w2s_train_sampled, y_w2s_train_sampled, x_test, y_test, verbose=verbose)\n",
    "\n",
    "            proportion_list = np.arange(0, 1.01, 0.1)\n",
    "            overlap_indices_full = np.arange(len(x_overlap))\n",
    "            nonoverlap_indices_full = np.arange(len(x_nonoverlap))\n",
    "            for overlap_portion in tqdm(proportion_list):\n",
    "                overlap_portion = np.round(overlap_portion, 1)\n",
    "                nonoverlap_size = int(wl_dataset_size * (1-overlap_portion))\n",
    "                overlap_size = int(wl_dataset_size * (overlap_portion))\n",
    "\n",
    "                nonoverlap_indices = np.random.choice(nonoverlap_indices_full, size=nonoverlap_size, replace=False).tolist()\n",
    "                overlap_indices = np.random.choice(overlap_indices_full, size=overlap_size, replace=False).tolist()\n",
    "                \n",
    "                sample_indices = nonoverlap_indices + overlap_indices\n",
    "                random.shuffle(sample_indices)\n",
    "\n",
    "                x_w2s = np.concatenate([x_nonoverlap[nonoverlap_indices], x_overlap[overlap_indices]])\n",
    "                y_w2s = np.concatenate([y_nonoverlap_pseudo[nonoverlap_indices], y_overlap_pseudo[overlap_indices]])\n",
    "                \n",
    "                # Replace LGBMClassifier with train_dnn\n",
    "                model, acc = train_dnn(x_w2s, y_w2s, x_test, y_test, verbose=verbose)\n",
    "                acc_list.append(acc)\n",
    "\n",
    "            result = {\n",
    "                'dataset_name': dataset_name,\n",
    "                'seed': seed,\n",
    "                'lm_acc': lm_acc,\n",
    "                'gt_acc': gt_acc,\n",
    "                'acc_list': acc_list,\n",
    "                'overlap_avg_acc': overlap_avg_acc,\n",
    "                'overlap_avg_confidence': overlap_avg_confidence,\n",
    "                'nonoverlap_avg_acc': nonoverlap_avg_acc,\n",
    "                'nonoverlap_avg_confidence': nonoverlap_avg_confidence,\n",
    "            }\n",
    "            print(result)\n",
    "            df_result.append(result)\n",
    "\n",
    "            plt.axhline(y=lm_acc, color='b', linestyle='--', label='weak')\n",
    "            plt.axhline(y=gt_acc, color='r', linestyle='--', label='strong (gt)')\n",
    "            plt.plot(proportion_list, acc_list, 'o-', label='strong (weak labels)')\n",
    "            plt.xlabel('Proportion of overlap density')\n",
    "            plt.ylabel('Acc')\n",
    "            plt.title(dataset_name+f' seed: {seed}')\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            if not os.path.exists(f'figures/dnn_change_point_detection_no_hard_normalized'):\n",
    "                os.makedirs(f'figures/dnn_change_point_detection_no_hard_normalized')\n",
    "            plt.savefig(f'figures/dnn_change_point_detection_no_hard_normalized/{dataset_name}.pdf', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(dataset_name, seed, 'failed')\n",
    "\n",
    "df_result = pd.DataFrame(df_result)\n",
    "df_result.to_csv('results/dnn_change_point_detection_no_hard_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_indices = np.where(align_scores >= align_score_threshold)[0]\n",
    "nonoverlap_indices = np.where(align_scores < align_score_threshold)[0]\n",
    "overlap_indices.shape, nonoverlap_indices.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_indices_full.shape, nonoverlap_indices_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(activations_easy_or_overlap_normalized @ activations_hard_points_normalized.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_hard_points_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
